### 项目中的数据怎么采集到kafka的，实现方式？

```
在项目中通过直连方式，采集到kafka中，flume中的sink直接指定到kafka中
```

### flume管道内存，flume宕机了数据丢失怎么解决(京东)

```
1、Flume的channel分为很多种，可以将数据写入到文件（file channel）
2、防止非首个agent宕机的方法数可以做集群或者主备
```

### flume配置方式，flume集群

```
这种情况应用的场景比较多，比如要收集Web网站的用户行为日志，Web网站为了可用性使用的负载均衡的集群模式，每个节点都产生用户行为日志，可以为每个节点都配置一个Agent来单独收集日志数据，然后多个Agent将数据最终汇聚到一个用来存储数据存储系统，如HDFS上。
```

### flume不采集Nginx日志，通过Logger4j采集日志，优缺点是什么？

```
优点：Nginx的日志格式是固定的，但是缺少sessionid，通过logger4j采集的日志是带
sessionid的，而session可以通过redis共享，保证了集群日志中的同一session落到不同的tomcat时，sessionId还是一样的，而且logger4j的方式比较稳定，不会宕机。
缺点：不够灵活，logger4j的方式和项目结合过于紧密，而flume的方式比较灵活，拔插式比较好，不会影响项目性能。
```

### flume和kafka采集日志区别，采集日志时中间停了，怎么记录之前的日志

```
Flume采集日志是通过流的方式直接将日志收集到存储层，而kafka试讲日志缓存在kafka集群，待后期可以采集到存储层。
Flume采集中间停了，可以采用文件的方式记录之前的日志，而kafka是采用offset的方式记录之前的日志。
```

### flume的极限是多大?(event数量)

```
一个最大是2048字节，在channel中默认1000000 
```

### Flume的负载均衡和高可用以及数据重复丢失问题

![这里写图片描述](相关知识点.assets/20160416002905893)

```
source里的event流经channel，进入sink组，在sink组内部根据负载算法（round_robin、random）选择sink，后续可以选择不同机器上的agent实现负载均衡。
```

### Flume数据丢失问题

当节点宕机有可能数据丢失，所以自定义source然后记录偏移量

```java
//自定义source，记录偏移量
    /*
    flume的生命周期： 先执行构造器，再执行 config方法 -> start方法-》 processor.process
    1、读取配置文件:(配置读取的文件内容：读取那个文件，编码及、偏移量写到那个文件，多长时间检测一下文件是否有新内容
    )
     */
自定义source然后打包，放在lib下
```









